---
title: "House_Prices"
author: "Sungwan Kim"
date: "9/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(Amelia)
library(rpart)
library(glmnet)
```

```{r, message = FALSE}
train <- read.csv('train.csv', stringsAsFactors = F)
test <- read.csv('test.csv', stringsAsFactors = F)
test$SalePrice <- rep(NA, 1459)
df <- rbind(train, test)
dim(df)
str(df)
```


We are dealing with high-dimensional data and this implies that there is a danger of overfitting.

The big part of this project will be dealing with all the missing values in the data. Depending on what method we use, there will be huge variability in precision of prediction accuracy. With so many missing values it is not advisable to drop all the NAs which will leave us with reduced sample size.

```{r}
missmap(df[,1:80],
        main = "Missing values in Housing Prices Dataset",
        y.labels = NULL,
        y.at = NULL)

sort(sapply(df[,1:80], function(x) sum(is.na(x))), decreasing = T)
```

## Missing Value Imputation

Dealing with missing value is not so trivial. The easiest case to deal with missing value would be to treat them as missing at random and drop them. However, it is hard to prove that data are missing at random. Given the nature of this data set, my intuition tells me that people did their due dilligence while collecting this data since houses are significant assets in average households.

Mean value substitution is a statistical malpractice. Even though it has desirable feature such as that it does not change the mean, it is not advisable to use it unless has a strong reason to do so. Given that, we will have to look at all the varialbles with missing values however painstaking it may be.

Not all variables are informative in predicting the missing values. While data can be missing due to mistakes in data imputation, we cannot dimiss a possibility that missing values are missing because they are truly not applicable. With these considerations in mind, let's look at all the missing values one by one.

I have been inspired by kernels on Kaggle especially for missing value imputation. You will notice that I followed closely for majority of the variables because I did not want to reinvent the wheels. However, I have looked closely at the assumptions made and see if there were better ways impute NAs. I am in no way claiming that my work is superior to that of others. I really appreciate all the hard works they have put into thinking and analyzing this data set. While I believe that other people used valid methods on sound assumptions, I did not hesitate to change things up when I believed there is a better way to do things in my opinion.

```{r}
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[which(df$PoolArea == 0), 'PoolQC'] <- 'None'
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[, c('PoolQC', 'PoolArea')] %>% 
  group_by(PoolQC) %>% 
  summarise(median = median(PoolArea), count = n())
df[c(2421, 2504), 'PoolQC'] <- 'Ex'
df[2600, 'PoolQC'] <- 'Fa'
sum(is.na(df$PoolQC))
```
```{r}
ggplot(df, aes(MiscFeature)) + geom_bar()
table(df$MiscFeature)
df[which(is.na(df$MiscFeature)), 'MiscFeature'] <- 'None'
sum(is.na(df$MiscFeature))
```

```{r}
table(df$Alley)
df[which(is.na(df$Alley)), 'Alley'] <- 'None'
sum(is.na(df$Alley))
```

```{r}
table(df$Fence)
df[which(is.na(df$Fence)), 'Fence'] <- 'None'
sum(is.na(df$Fence))
```

```{r}
table(df$FireplaceQu)
df[which(is.na(df$FireplaceQu)), c('FireplaceQu', 'Fireplaces')]
df[which(df$Fireplaces == 0), 'FireplaceQu'] <- 'None'
sum(is.na(df$FireplaceQu))
```

We have to make a judgement call as missing values constituted to about 15% of the number of observations. 
```{r}
ggplot(df, aes(LotFrontage)) + geom_density()

df$Imputed <- 'Original'
df[which(is.na(df$LotFrontage)),]$Imputed <- 'Imputed'

lot.rpart <- rpart(LotFrontage ~ LotArea + LotShape + LotConfig, data = df[!is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')], method = "anova", na.action=na.omit)

df$LotFrontage[is.na(df$LotFrontage)] <- round(predict(lot.rpart, df[is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')]))

ggplot(df, aes(LotFrontage, color = Imputed)) + geom_density()

df <- df[, -81]

sum(is.na(df$LotFrontage))
```

```{r}
df[which(is.na(df$GarageYrBlt)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]
df[which(df$GarageArea == 0),]$GarageType <- 'None'
df[which(df$GarageArea == 0),]$GarageFinish <- 'None'
df[which(df$GarageArea == 0),]$GarageQual <- 'None'
df[which(df$GarageArea == 0),]$GarageCond <- 'None'

sum(df$GarageYrBlt == df$YearBuilt, na.rm = T)
df[which(is.na(df$GarageYrBlt)),]$GarageYrBlt <- df[which(is.na(df$GarageYrBlt)),]$YearBuilt

df[which(is.na(df$GarageFinish)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]

table(as.factor(df$GarageFinish[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
df[2127, 'GarageFinish'] <- 'Unf'
df[2127, 'GarageQual'] <- 'TA'
df[2127, 'GarageCond'] <- 'Fa'

df[2577, 'GarageFinish'] <- 'None'
df[2577, 'GarageCars'] <- 0
df[2577, 'GarageArea'] <- 0
df[2577, 'GarageQual'] <- 'None'
df[2577, 'GarageCond'] <- 'None'
```

```{r}
df[which(is.na(df$BsmtExposure)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtQual <- 'None'
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtCond <- 'None'
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtExposure <- 'None'
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtFinType1 <- 'None'
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtFinType2 <- 'None'
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtUnfSF <- 0
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$TotalBsmtSF <- 0
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtFullBath <- 0
df[which(df$BsmtFinSF1 == 0 & df$BsmtFinSF2 == 0),]$BsmtHalfBath <- 0
table(df$BsmtExposure)
df[c(949, 1488, 2349),]$BsmtExposure <- 'No'
df[which(is.na(df$BsmtCond)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]
```

```{r}
df[which(is.na(df$MasVnrType)), c('MasVnrType', 'MasVnrArea')]
df[which(is.na(df$MasVnrArea)),]$MasVnrType <- 'None'
df[which(is.na(df$MasVnrArea)),]$MasVnrArea <- 0
df[, c('MasVnrType', 'MasVnrArea')] %>% 
  group_by(MasVnrType) %>% 
  summarise(median = median(MasVnrArea), count = n())
df[2611, ]$MasVnrType <- 'BrkFace'
```

```{r}
df[which(is.na(df$MSZoning)), c('MSZoning', 'MSSubClass')]
df[, c('MSZoning', 'MSSubClass')] %>% 
  group_by(MSZoning) %>% 
  summarise(mean = mean(MSSubClass), count = n())
df[c(1916, 2217, 2905),]$MSZoning <- 'RL'
df[2251,]$MSZoning <- 'RM'
```

```{r}
table(df$Utilities)
df[which(is.na(df$Utilities)),]$Utilities <- 'AllPub'
```

```{r}
table(df$Functional)
df[which(is.na(df$Functional)),]$Functional <- 'Typ'
```

```{r}
df[which(is.na(df$Exterior1st) | is.na(df$Exterior2nd)), c('Exterior1st', 'Exterior2nd')]
table(df$Exterior1st)
table(df$Exterior2nd)
df[which(is.na(df$Exterior1st)),]$Exterior1st <- 'VinylSd'
df[which(is.na(df$Exterior2nd)),]$Exterior2nd <- 'VinylSd'
```

```{r}
table(df$SaleType)
df[which(is.na(df$SaleType)),]$SaleType <- 'WD'
```

```{r}
table(df$Electrical)
df[which(is.na(df$Electrical)),]$Electrical <- 'SBrkr'
```

```{r}
table(as.factor(df$KitchenQual))
df[which(is.na(df$KitchenQual)),]$KitchenQual <- 'TA'
```

```{r}
sort(sapply(df[,1:80], function(x) sum(is.na(x))), decreasing = T)
```

##Subset Selection

## Shrinkage Methods

### Ridge Regression and the Lasso

```{r}
x <- model.matrix(SalePrice~.,train)
y <- train$SalePrice

ridge.mod <- glmnet(x, y, alpha = 0, lambda)
```


## Dimension Reduction Methods

### Principal Components Regression (PCA)
