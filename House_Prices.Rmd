---
title: "House_Prices"
author: "Sungwan Kim"
date: "9/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(Amelia)
library(rpart)
library(leaps)
library(glmnet)
library(caret)
library(pls)
```

```{r, message = FALSE}
train <- read.csv('train.csv', stringsAsFactors = F)
test <- read.csv('test.csv', stringsAsFactors = F)
test$SalePrice <- rep(NA, 1459)
df <- rbind(train, test)
dim(df)
str(df)
```

The big part of this project will be dealing with all the missing values in the data. Depending on what method we use, there will be huge variability in precision of prediction accuracy. With so many missing values it is not advisable to drop all the NAs which will leave us with reduced sample size.

```{r}
missmap(df[,1:80],
        main = "Missing values in Housing Prices Dataset",
        y.labels = NULL,
        y.at = NULL)

sort(sapply(df[,1:80], function(x) sum(is.na(x))), decreasing = T)
```

## Missing Value Imputation

Dealing with missing value is not so trivial. The easiest case to deal with missing value would be to treat them as missing at random and drop them. However, it is hard to prove that data are missing at random. Given the nature of this data set, my intuition is that the agents did their due dilligence while collecting this data since houses are significant assets in average households.

Mean value substitution is a statistical malpractice. Even though it has desirable feature such as that it does not change the mean, it is not advisable to use it unless has a strong reason to do so. Given that, we will have to look at all the varialbles with missing values one by one however painstaking it may be.

Not all variables are informative in predicting the missing values. While data can be missing due to mistakes in data imputation, we cannot dimiss a possibility that missing values are missing because they are truly not applicable. With these considerations in mind, let's look at all the missing values one by one.

I have been inspired by kernels on Kaggle especially for missing value imputation. You will notice that I have followed closely for majority of the variables since I did not want to reinvent the wheels. However, I have looked closely at the assumptions made and saw if there were better ways to impute the NAs. I am in no way claiming that my work is superior to that of others'. I really appreciate all the hard works they have put into thinking and analyzing this data set. While I believe that other people used valid methods on sound assumptions, I did not hesitate to make judgment calls whenever necessary. The two most influential kernels were:

https://www.kaggle.com/tannercarbonati/detailed-data-analysis-ensemble-modeling
https://www.kaggle.com/bisaria/handling-missing-data

Missing value imputation for variable is reasonable when 5% of the observations are missing unless there is a specific reason to believe otherwise. The problem is we have variables where majority of the observations are missing, so we have to make a judgement call whether to salvage those variables. This is one of those special cases when data are missing for valid reason so we save them. 

```{r}
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[which(df$PoolArea == 0), 'PoolQC'] <- 'None'
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[, c('PoolQC', 'PoolArea')] %>% 
  group_by(PoolQC) %>% 
  summarise(median = median(PoolArea), count = n())
df[c(2421, 2504), 'PoolQC'] <- 'Ex'
df[2600, 'PoolQC'] <- 'Fa'
```
```{r}
table(df$MiscFeature)
df[which(is.na(df$MiscFeature)), 'MiscFeature'] <- 'None'
```

```{r}
table(df$Alley)
df[which(is.na(df$Alley)), 'Alley'] <- 'None'
```

```{r}
table(df$Fence)
df[which(is.na(df$Fence)), 'Fence'] <- 'None'
```

```{r}
df[which(is.na(df$FireplaceQu)), c('FireplaceQu', 'Fireplaces')]
df[which(df$Fireplaces == 0), 'FireplaceQu'] <- 'None'
```

We have to make a judgement call as missing values constituted to about 15% of the number of observations. 

```{r}
ggplot(df, aes(LotFrontage)) + geom_density()

df$Imputed <- 'Original'
df[which(is.na(df$LotFrontage)),]$Imputed <- 'Imputed'

lot.rpart <- rpart(LotFrontage ~ LotArea + LotShape + LotConfig, data = df[!is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')], method = "anova", na.action=na.omit)

df$LotFrontage[is.na(df$LotFrontage)] <- round(predict(lot.rpart, df[is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')]))

ggplot(df, aes(LotFrontage, color = Imputed)) + geom_density()

df <- df[, -82]

sum(is.na(df$LotFrontage))
```

```{r}
df[which(is.na(df$GarageYrBlt)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]
df[which(df$GarageArea == 0),]$GarageType <- 'None'
df[which(df$GarageArea == 0),]$GarageFinish <- 'None'
df[which(df$GarageArea == 0),]$GarageQual <- 'None'
df[which(df$GarageArea == 0),]$GarageCond <- 'None'

sum(df$GarageYrBlt == df$YearBuilt, na.rm = T)
df[which(is.na(df$GarageYrBlt)),]$GarageYrBlt <- df[which(is.na(df$GarageYrBlt)),]$YearBuilt

df[which(is.na(df$GarageFinish)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]

table(as.factor(df$GarageFinish[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
df[2127, 'GarageFinish'] <- 'Unf'
df[2127, 'GarageQual'] <- 'TA'
df[2127, 'GarageCond'] <- 'Fa'

df[2577, 'GarageFinish'] <- 'None'
df[2577, 'GarageCars'] <- 0
df[2577, 'GarageArea'] <- 0
df[2577, 'GarageQual'] <- 'None'
df[2577, 'GarageCond'] <- 'None'
```

```{r}
df[which(is.na(df$BsmtExposure) | is.na(df$BsmtCond)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtQual <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtCond <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtExposure <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFinType1 <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFinType2 <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtUnfSF <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$TotalBsmtSF <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFullBath <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtHalfBath <- 0
df[(which(is.na(df$BsmtFinSF1))),]$BsmtFinSF1 <- 0
df[(which(is.na(df$BsmtFinSF2))),]$BsmtFinSF2 <- 0

df[which(is.na(df$BsmtExposure) | is.na(df$BsmtCond)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]

col.pred <- c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')
BsmtQual.rpart <- rpart(as.factor(BsmtQual) ~ .,
                           data = df[!is.na(df$BsmtQual),col.pred], 
                           method = "class", 
                           na.action = na.omit)
df$BsmtQual[is.na(df$BsmtQual)] <- as.character(predict(BsmtQual.rpart, 
                                           df[is.na(df$BsmtQual), col.pred], 
                                           type = "class"))
BsmtCond.rpart <- rpart(as.factor(BsmtCond) ~ .,
                           data = df[!is.na(df$BsmtCond),col.pred], 
                           method = "class", 
                           na.action = na.omit)
df$BsmtCond[is.na(df$BsmtCond)] <- as.character(predict(BsmtCond.rpart, 
                                           df[is.na(df$BsmtCond), col.pred], 
                                           type = "class"))
BsmtExposure.rpart <- rpart(as.factor(BsmtExposure) ~ .,
                           data = df[!is.na(df$BsmtExposure),col.pred], 
                           method = "class", 
                           na.action=na.omit)
df$BsmtExposure[is.na(df$BsmtExposure)] <- as.character(predict(BsmtExposure.rpart,       
                                      df[is.na(df$BsmtExposure),col.pred], 
                                      type="class"))
BsmtFinType2.rpart <- rpart(as.factor(BsmtFinType2) ~ .,
                           data = df[!is.na(df$BsmtFinType2),col.pred], 
                           method = "class", 
                           na.action=na.omit)
df$BsmtFinType2[is.na(df$BsmtFinType2)] <- as.character(predict(BsmtFinType2.rpart,       
                                      df[is.na(df$BsmtFinType2),col.pred], 
                                      type="class"))
```

```{r}
df[which(is.na(df$MasVnrType)), c('MasVnrType', 'MasVnrArea')]
df[which(is.na(df$MasVnrArea)),]$MasVnrType <- 'None'
df[which(is.na(df$MasVnrArea)),]$MasVnrArea <- 0
df[, c('MasVnrType', 'MasVnrArea')] %>% 
  group_by(MasVnrType) %>% 
  summarise(median = median(MasVnrArea), count = n())
df[2611, ]$MasVnrType <- 'BrkFace'
```

```{r}
df[which(is.na(df$MSZoning)), c('MSZoning', 'MSSubClass')]
df[, c('MSZoning', 'MSSubClass')] %>% 
  group_by(MSZoning) %>% 
  summarise(mean = mean(MSSubClass), count = n())
df[c(1916, 2217, 2905),]$MSZoning <- 'RL'
df[2251,]$MSZoning <- 'RM'
```

There is only one value of NoSeWa in Utilities and this is contained only in the training set, so we drop the whole variable as this is not going to helpful when predicting SalePrice for the testing set. This is a very important point, and we will come back to this after we are done dealing with missing values.

```{r}
table(df$Utilities)
which(df$Utilities == 'NoSeWa') # in the training data set
df <- df[,!names(df) %in% c('Utilities')]
```

```{r}
table(df$Functional)
df[which(is.na(df$Functional)),]$Functional <- 'Typ'
```

```{r}
df[which(is.na(df$Exterior1st) | is.na(df$Exterior2nd)), c('Exterior1st', 'Exterior2nd')]
table(df$Exterior1st)
table(df$Exterior2nd)
df[which(is.na(df$Exterior1st)),]$Exterior1st <- 'VinylSd'
df[which(is.na(df$Exterior2nd)),]$Exterior2nd <- 'VinylSd'
```

```{r}
table(df$SaleType)
df[which(is.na(df$SaleType)),]$SaleType <- 'WD'
```

```{r}
table(df$Electrical)
df[which(is.na(df$Electrical)),]$Electrical <- 'SBrkr'
```

```{r}
table(as.factor(df$KitchenQual))
df[which(is.na(df$KitchenQual)),]$KitchenQual <- 'TA'
```

```{r}
sort(sapply(df[,1:79], function(x) sum(is.na(x))), decreasing = T)
```

```{r}
sum(is.na(df))
train <- df[1:1460, -1]
test <- df[1461:2919, -1]
```

While there remains no missing value in the data set, we are not done yet. Remember the Utilities variable? There are categorical variables with more levels for training set than testing set. We will look at each and reduce the levels down to match that of the testing set. We will follow precedent and sensibly impute data whether it be using the most frequent value, other related variables, or just common sense.

```{r}
cat_features <- names(which(sapply(train, is.character)))
train[cat_features] <- lapply(train[cat_features] , factor)
test[cat_features] <- lapply(test[cat_features], factor)

match(lapply(train[cat_features], levels), lapply(test[cat_features], levels))
lapply(train[cat_features], levels)[[10]]
lapply(test[cat_features], levels)[[10]]
cat_features[10]
table(train$Condition2)
table(test$Condition2)
train[which(df$Condition2 %in% c("RRAe", "RRAn", "RRNn")),]$Condition2 <- "Norm"
train$Condition2 <- factor(train$Condition2)

lapply(train[cat_features], levels)[[12]]
lapply(test[cat_features], levels)[[12]]
cat_features[12]
table(train$HouseStyle)
table(test$HouseStyle)
train[which(df$HouseStyle == "2.5Fin"),]$HouseStyle <- "2Story"
train$HouseStyle <- factor(train$HouseStyle)

lapply(train[cat_features], levels)[[14]]
lapply(test[cat_features], levels)[[14]]
cat_features[14]
table(train$RoofMatl)
table(test$RoofMatl)
train[which(df$RoofMatl %in% c("ClyTile", "Membran", "Metal", "Roll")),]$RoofMatl <- "CompShg"
train$RoofMatl <- factor(train$RoofMatl)

lapply(train[cat_features], levels)[[15]]
lapply(test[cat_features], levels)[[15]]
cat_features[15]
table(train$Exterior1st)
table(test$Exterior1st)
train[, c('Exterior1st', 'SalePrice')] %>% 
  group_by(Exterior1st) %>% 
  summarise(mean = mean(SalePrice), count = n())
train[which(df$Exterior1st %in% c("ImStucc", "Stone")),]$Exterior1st <- "CemntBd"
train$Exterior1st <- factor(train$Exterior1st)

lapply(train[cat_features], levels)[[16]]
lapply(test[cat_features], levels)[[16]]
cat_features[16]
table(train$Exterior2nd)
table(test$Exterior2nd)
train[, c('Exterior2nd', 'SalePrice')] %>% 
  group_by(Exterior2nd) %>% 
  summarise(mean = mean(SalePrice), count = n())
train[which(df$Exterior2nd == "Other"),]$Exterior2nd <- "VinylSd"
train$Exterior2nd <- factor(train$Exterior2nd)

lapply(train[cat_features], levels)[[26]]
lapply(test[cat_features], levels)[[26]]
cat_features[26]
table(train$Heating)
table(test$Heating)
train[which(df$Heating %in% c("Floor", "OthW")),]$Heating <- "GasA"
train$Heating <- factor(train$Heating)

lapply(train[cat_features], levels)[[29]]
lapply(test[cat_features], levels)[[29]]
cat_features[29]
table(train$Electrical)
table(test$Electrical)
train[which(df$Electrical == "Mix"),]$Electrical <- "SBrkr"
train$Electrical <- factor(train$Electrical)

lapply(train[cat_features], levels)[[40]]
lapply(test[cat_features], levels)[[40]]
cat_features[40]
table(train$MiscFeature)
table(test$MiscFeature)
train[, c('MiscFeature', 'SalePrice')] %>% 
  group_by(MiscFeature) %>% 
  summarise(mean = mean(SalePrice), count = n())
train[which(df$MiscFeature == "TenC"),]$MiscFeature <- "None"
train$MiscFeature <- factor(train$MiscFeature)
match(lapply(train[cat_features], levels), lapply(test[cat_features], levels))
```


## Linear Regression

Simple linear regression is historically designed for cases when the number of observations is much larger than the number of variables (i.e. n >> p). However, if n is not much greater than p, then least square fit might suffer from high variance resulting in poor predictions due to overfitting. Fortunately, there are multiple methods for treating overfitting in this case to increase prediction accuracy. 

```{r}
lm.mod <- lm(SalePrice ~ ., train)
summary(lm.mod)
pred <- predict(lm.mod, test, type = "response")
submit <- data.frame(Id = 1461:2919, SalePrice = pred)
write.csv(submit, file = "lm.mod.csv", row.names = FALSE)
```

0.18707 0.16582


## Subset Selection Methods

Our main focus from here on would be to explore possible options to deal with overfitting which leads to high predictive accuracy.

The limitation of the best subset selection is that when the number of variables p becomes very large, the computation becomes infeasible. I believe the rule of thumb is more than 40 variables, but we have 243 meaning the computer has to calculate 2^243 possible models! Why do we have 243 variables? It is because dummy variables are created for categorical variables which have value 1 for that specific value and 0 for others. Therefore, the true number of variables is the number of numerical variables plus the number of all the levels of factor variables.

```{r, error = TRUE}
regfit.full <- regsubsets(SalePrice ~ ., train)
```

Fortunately, there is an alternative option of stepwise selection.

### Forward Stepwise Selection

Forward stepwise selection starts with a model with no regressors and subsequently add one variable that results in the greatest improvement to the fit. The advantage over the best subset selection is that the number of models that need to be calculated is pared down to `r 243(243 + 1)/2 + 1`. Forward stepwise selection is different from best subset selection in that if the variable is added in the previous steps, it cannot be taken out in the subsequent steps even if it may not be optimal selection for the model with specific number of features.

```{r}
regfit.fwd <- regsubsets(SalePrice ~ ., data = train, nvmax = NULL, method = "forward")
reg.summary.fwd <- summary(regfit.fwd)
par(mfrow = c(2, 2))
plot(reg.summary.fwd$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary.fwd$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary.fwd$adjr2)
points(155, reg.summary.fwd$adjr2[155], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary.fwd$cp)
points(101, reg.summary.fwd$cp[101], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary.fwd$bic)
points(47, reg.summary.fwd$bic[47], col = "red", cex = 2, pch = 20)

test$SalePrice <- 0
x.test = model.matrix(SalePrice ~ ., data = test)
coefi = coef(regfit.fwd, id = 47)
pred = x.test[, names(coefi)] %*% coefi
submit <- data.frame(Id = 1461:2919, SalePrice = pred)
which(submit$SalePrice < 0)
submit[1412,]$SalePrice <- -submit[1412,]$SalePrice
write.csv(submit, file = "forward.csv", row.names = FALSE)
```

0.22840

### Backward Stepwise Selection

Backward stepwise selection works the exact opposite starting with a model with all the variables and subsequently drops a variable that is least important in terms of fit. In high dimensional settings where p > n backward stepwise selection cannot be used, but we fortunately does not suffer from this situation so we can use it. 

```{r}
regfit.bwd <- regsubsets(SalePrice ~ ., data = train, nvmax = NULL, method = "backward")
reg.summary.bwd <- summary(regfit.bwd)
par(mfrow = c(2, 2))
plot(reg.summary.bwd$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary.bwd$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary.bwd$adjr2)
points(137, reg.summary.bwd$adjr2[137], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary.bwd$cp)
points(106, reg.summary.bwd$cp[106], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary.bwd$bic)
points(43, reg.summary.bwd$bic[43], col = "red", cex = 2, pch = 20)

x.test = model.matrix(SalePrice ~ ., data = test)
coefi = coef(regfit.fwd, id = 43)
pred = x.test[, names(coefi)] %*% coefi
submit <- data.frame(Id = 1461:2919, SalePrice = pred)
which(submit$SalePrice < 0)
submit[757,]$SalePrice <- -submit[757,]$SalePrice
write.csv(submit, file = "backward.csv", row.names = FALSE)
```

0.21383

We get a widely varying number of best features depending on metrics we use. However, I performed prediction only on the lowest values for forward and backward stepwise selection as there seems to be not much difference for values of Cp and Adjusted RSq for the lowest to the highest number.


## Shrinkage Methods

### Ridge Regression

Next, we will consider ridge regression which constrains the coefficients towards zero. When estimating regression coefficients, we add a shrinkage penalty that penalizes the large values of coefficients effectively bringing down the coefficients near zero. Alpha = 0 means ridge while alpha = 1 means lasso regression. To use the best tuning parameter lambda, we will use cross-validation.

```{r}
glm.cv.ridge <- cv.glmnet(data.matrix(train)[,-c(1,79)], train$SalePrice, alpha = 0)
(lambda.ridge <- glm.cv.ridge$lambda.min)
glm.ridge <- glmnet(x = data.matrix(train)[,-c(1,79)], y = train$SalePrice, alpha = 0, lambda = lambda.ridge )
test$SalePrice <- NULL
y_pred.ridge <- as.double(predict(glm.ridge, data.matrix(test)[,-1]))
submit <- data.frame(Id = 1461:2919, SalePrice = y_pred.ridge)
write.csv(submit, file = "ridge.csv", row.names = FALSE)
```
0.15839

### Lasso Regression

```{r}
glm.cv.lasso <- cv.glmnet(data.matrix(train)[,-c(1,79)], train$SalePrice, alpha = 1)
(lambda.lasso <- glm.cv.lasso$lambda.min)
glm.lasso <- glmnet(x = data.matrix(train)[,-c(1,79)], y = train$SalePrice, alpha = 1, lambda = lambda.lasso)
y_pred.lasso <- as.double(predict(glm.lasso, data.matrix(test)[,-1]))
submit <- data.frame(Id = 1461:2919, SalePrice = y_pred.lasso)
write.csv(submit, file = "lasso.csv", row.names = FALSE)
```
0.16465
