---
title: "House_Prices"
author: "Sungwan Kim"
date: "9/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(Amelia)
library(rpart)
library(leaps)
library(glmnet)
```

```{r, message = FALSE}
train <- read.csv('train.csv', stringsAsFactors = F)
test <- read.csv('test.csv', stringsAsFactors = F)
test$SalePrice <- rep(NA, 1459)
df <- rbind(train, test)
dim(df)
str(df)
```


We are dealing with high-dimensional data and this implies that there is a danger of overfitting.

The big part of this project will be dealing with all the missing values in the data. Depending on what method we use, there will be huge variability in precision of prediction accuracy. With so many missing values it is not advisable to drop all the NAs which will leave us with reduced sample size.

```{r}
missmap(df[,1:80],
        main = "Missing values in Housing Prices Dataset",
        y.labels = NULL,
        y.at = NULL)

sort(sapply(df[,1:80], function(x) sum(is.na(x))), decreasing = T)
```

## Missing Value Imputation

Dealing with missing value is not so trivial. The easiest case to deal with missing value would be to treat them as missing at random and drop them. However, it is hard to prove that data are missing at random. Given the nature of this data set, my intuition tells me that people did their due dilligence while collecting this data since houses are significant assets in average households.

Mean value substitution is a statistical malpractice. Even though it has desirable feature such as that it does not change the mean, it is not advisable to use it unless has a strong reason to do so. Given that, we will have to look at all the varialbles with missing values however painstaking it may be.

Not all variables are informative in predicting the missing values. While data can be missing due to mistakes in data imputation, we cannot dimiss a possibility that missing values are missing because they are truly not applicable. With these considerations in mind, let's look at all the missing values one by one.

I have been inspired by kernels on Kaggle especially for missing value imputation. You will notice that I followed closely for majority of the variables because I did not want to reinvent the wheels. However, I have looked closely at the assumptions made and see if there were better ways impute NAs. I am in no way claiming that my work is superior to that of others. I really appreciate all the hard works they have put into thinking and analyzing this data set. While I believe that other people used valid methods on sound assumptions, I did not hesitate to change things up when I believed there is a better way to do things in my opinion.

```{r}
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[which(df$PoolArea == 0), 'PoolQC'] <- 'None'
df[which(is.na(df$PoolQC)), c('PoolQC', 'PoolArea')]
df[, c('PoolQC', 'PoolArea')] %>% 
  group_by(PoolQC) %>% 
  summarise(median = median(PoolArea), count = n())
df[c(2421, 2504), 'PoolQC'] <- 'Ex'
df[2600, 'PoolQC'] <- 'Fa'
```
```{r}
table(df$MiscFeature)
df[which(is.na(df$MiscFeature)), 'MiscFeature'] <- 'None'
```

```{r}
table(df$Alley)
df[which(is.na(df$Alley)), 'Alley'] <- 'None'
```

```{r}
table(df$Fence)
df[which(is.na(df$Fence)), 'Fence'] <- 'None'
```

```{r}
df[which(is.na(df$FireplaceQu)), c('FireplaceQu', 'Fireplaces')]
df[which(df$Fireplaces == 0), 'FireplaceQu'] <- 'None'
```

We have to make a judgement call as missing values constituted to about 15% of the number of observations. 
```{r}
ggplot(df, aes(LotFrontage)) + geom_density()

df$Imputed <- 'Original'
df[which(is.na(df$LotFrontage)),]$Imputed <- 'Imputed'

lot.rpart <- rpart(LotFrontage ~ LotArea + LotShape + LotConfig, data = df[!is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')], method = "anova", na.action=na.omit)

df$LotFrontage[is.na(df$LotFrontage)] <- round(predict(lot.rpart, df[is.na(df$LotFrontage), c('LotFrontage', 'LotArea', 'LotShape', 'LotConfig')]))

ggplot(df, aes(LotFrontage, color = Imputed)) + geom_density()

df <- df[, -82]

sum(is.na(df$LotFrontage))
```

```{r}
df[which(is.na(df$GarageYrBlt)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]
df[which(df$GarageArea == 0),]$GarageType <- 'None'
df[which(df$GarageArea == 0),]$GarageFinish <- 'None'
df[which(df$GarageArea == 0),]$GarageQual <- 'None'
df[which(df$GarageArea == 0),]$GarageCond <- 'None'

sum(df$GarageYrBlt == df$YearBuilt, na.rm = T)
df[which(is.na(df$GarageYrBlt)),]$GarageYrBlt <- df[which(is.na(df$GarageYrBlt)),]$YearBuilt

df[which(is.na(df$GarageFinish)), c('GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond')]

table(as.factor(df$GarageFinish[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
table(as.factor(df$GarageQual[df$GarageType == 'Detchd' & df$GarageYrBlt == 1910]))
df[2127, 'GarageFinish'] <- 'Unf'
df[2127, 'GarageQual'] <- 'TA'
df[2127, 'GarageCond'] <- 'Fa'

df[2577, 'GarageFinish'] <- 'None'
df[2577, 'GarageCars'] <- 0
df[2577, 'GarageArea'] <- 0
df[2577, 'GarageQual'] <- 'None'
df[2577, 'GarageCond'] <- 'None'
```

```{r}
df[which(is.na(df$BsmtExposure) | is.na(df$BsmtCond)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtQual <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtCond <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtExposure <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFinType1 <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFinType2 <- 'None'
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtUnfSF <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$TotalBsmtSF <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtFullBath <- 0
df[c(which(df$TotalBsmtSF == 0), 2121),]$BsmtHalfBath <- 0
df[(which(is.na(df$BsmtFinSF1))),]$BsmtFinSF1 <- 0
df[(which(is.na(df$BsmtFinSF2))),]$BsmtFinSF2 <- 0

df[which(is.na(df$BsmtExposure) | is.na(df$BsmtCond)), c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')]

col.pred <- c('BsmtQual', 'BsmtCond', 'BsmtExposure',  'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath')
BsmtQual.rpart <- rpart(as.factor(BsmtQual) ~ .,
                           data = df[!is.na(df$BsmtQual),col.pred], 
                           method = "class", 
                           na.action = na.omit)
df$BsmtQual[is.na(df$BsmtQual)] <- as.character(predict(BsmtQual.rpart, 
                                           df[is.na(df$BsmtQual), col.pred], 
                                           type = "class"))
BsmtCond.rpart <- rpart(as.factor(BsmtCond) ~ .,
                           data = df[!is.na(df$BsmtCond),col.pred], 
                           method = "class", 
                           na.action = na.omit)
df$BsmtCond[is.na(df$BsmtCond)] <- as.character(predict(BsmtCond.rpart, 
                                           df[is.na(df$BsmtCond), col.pred], 
                                           type = "class"))
BsmtExposure.rpart <- rpart(as.factor(BsmtExposure) ~ .,
                           data = df[!is.na(df$BsmtExposure),col.pred], 
                           method = "class", 
                           na.action=na.omit)
df$BsmtExposure[is.na(df$BsmtExposure)] <- as.character(predict(BsmtExposure.rpart,       
                                      df[is.na(df$BsmtExposure),col.pred], 
                                      type="class"))
BsmtFinType2.rpart <- rpart(as.factor(BsmtFinType2) ~ .,
                           data = df[!is.na(df$BsmtFinType2),col.pred], 
                           method = "class", 
                           na.action=na.omit)
df$BsmtFinType2[is.na(df$BsmtFinType2)] <- as.character(predict(BsmtFinType2.rpart,       
                                      df[is.na(df$BsmtFinType2),col.pred], 
                                      type="class"))
```

```{r}
df[which(is.na(df$MasVnrType)), c('MasVnrType', 'MasVnrArea')]
df[which(is.na(df$MasVnrArea)),]$MasVnrType <- 'None'
df[which(is.na(df$MasVnrArea)),]$MasVnrArea <- 0
df[, c('MasVnrType', 'MasVnrArea')] %>% 
  group_by(MasVnrType) %>% 
  summarise(median = median(MasVnrArea), count = n())
df[2611, ]$MasVnrType <- 'BrkFace'
```

```{r}
df[which(is.na(df$MSZoning)), c('MSZoning', 'MSSubClass')]
df[, c('MSZoning', 'MSSubClass')] %>% 
  group_by(MSZoning) %>% 
  summarise(mean = mean(MSSubClass), count = n())
df[c(1916, 2217, 2905),]$MSZoning <- 'RL'
df[2251,]$MSZoning <- 'RM'
```

```{r}
table(df$Utilities)
df[which(is.na(df$Utilities)),]$Utilities <- 'AllPub'
```

```{r}
table(df$Functional)
df[which(is.na(df$Functional)),]$Functional <- 'Typ'
```

```{r}
df[which(is.na(df$Exterior1st) | is.na(df$Exterior2nd)), c('Exterior1st', 'Exterior2nd')]
table(df$Exterior1st)
table(df$Exterior2nd)
df[which(is.na(df$Exterior1st)),]$Exterior1st <- 'VinylSd'
df[which(is.na(df$Exterior2nd)),]$Exterior2nd <- 'VinylSd'
```

```{r}
table(df$SaleType)
df[which(is.na(df$SaleType)),]$SaleType <- 'WD'
```

```{r}
table(df$Electrical)
df[which(is.na(df$Electrical)),]$Electrical <- 'SBrkr'
```

```{r}
table(as.factor(df$KitchenQual))
df[which(is.na(df$KitchenQual)),]$KitchenQual <- 'TA'
```

```{r}
sort(sapply(df[,1:80], function(x) sum(is.na(x))), decreasing = T)
```

```{r}
sum(is.na(df))
train <- df[1:1460, -1]
test <- df[1461:2919, -1]
```

## Linear Regression

Least square regression is designed for n >> p when the number of observations is much larger than the number of variables. However, if n is not much greater than p, then least square fit might suffer from high variance resulting in poor predictions due to overfitting.

```{r}
lm.mod <- lm(SalePrice ~ ., train)
summary(lm.mod)
```


## Subset Selection Methods

The limitation of best subset selection is that when variables p becomes very large, the computation becomes infeasible. The rule of thumb is more than 40 variables and we have 80 meaning the computer has to calculate 2^80 different models!

```{r}
regfit.full <- regsubsets(SalePrice ~ ., train)
```

### Forward and Backward Stepwise Selection

Fortunately, there is an alternative option which is stepwise selection. The number of models calculated is `r 80(80 + 1)/2 + 1`. In high dimensional settings where p > n backward stepwise selection cannot be used, but we fortunately does not face this situation. 

```{r}
regfit.fwd <- regsubsets(SalePrice ~ ., data = train, nvmax = 80, method = "forward")
reg.summary.fwd <- summary(regfit.fwd)
reg.summary.fwd$rsq
par(mfrow = c(2, 2))
plot(reg.summary.fwd$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary.fwd$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary.fwd$adjr2)
points(81, reg.summary.fwd$adjr2[81], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary.fwd$cp)
points(81, reg.summary.fwd$cp[81], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary.fwd$bic)
points(47, reg.summary.fwd$bic[41], col = "red", cex = 2, pch = 20)

coef(regfit.fwd, 47)

regfit.bwd <- regsubsets(SalePrice ~ ., data = train, nvmax = 80, method = "backward")
reg.summary.bwd <- summary(regfit.bwd)
reg.summary.bwd$rsq
par(mfrow = c(2, 2))
plot(reg.summary.bwd$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary.bwd$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary.bwd$adjr2)
points(81, reg.summary.bwd$adjr2[81], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary.bwd$cp)
points(80, reg.summary.bwd$cp[80], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary.bwd$bic)
points(44, reg.summary.bwd$bic[44], col = "red", cex = 2, pch = 20)

coef(regfit.bwd, 44)
```

So, how do we choose the optimal model? We will use validation set and cross-validation approaches. I had to drop few variables with low variance as when I further split the training set to training observations and testing observations, some variables are factors with only one variable.

```{r}
set.seed(1)
train2 <- sample(c(TRUE, FALSE), nrow(train), rep = TRUE)
test2 <- (!train2)
regfit.fwd <- regsubsets(SalePrice ~ ., data = train[train2,], nvmax = 80, method = "forward")
test.mat <- model.matrix(SalePrice ~ ., data = train[test2,])
val.errors <- rep(NA, 80)
for (i in 80){
  coefi <- coef(regfit.fwd, id = i)
  pred <- test.mat[,names(coefi)]%*%coefi
  val.errors[i] <- mean((train$SalePrice[test2] - pred)^2)
}
val.errors
which.min(val.erros)
```

```{r}
k = 10
set.seed(1)
folds <- sample(1:k, nrow(train), replace = TRUE)
cv.errors <- matrix(NA, k, 80, dimnames = list(NULL, paste(1:80)))
for(j in 1:k){
  best.fit <- regsubsets(SalePrice ~.-Id, data = train[folds!=j,], nvmax = 80, method = "forward")
  for(i in 1:80){
    pred <- predict(best.fit, train[folds == j,], id = i)
    cv.errors[j,i] <- mean((train$SalePrice[folds == j] - pred)^2)
  }
}
```


## Shrinkage Methods

### Ridge Regression and the Lasso

```{r}
x <- model.matrix(SalePrice~.-Id, train)
y <- train$SalePrice
grid <- 10 ^ seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```


## Dimension Reduction Methods

### Principal Components Regression (PCA)
